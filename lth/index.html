<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Jaron Maene</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="canonical" href="http://maene.dev" />
  <link rel="stylesheet" href="../style.css">
</head>


<body>
  <p style="text-align:left;">
    <a href="../index.html">Home</a>
    <span style="float:right;color:gray;">
        Written June 16, 2021
    </span>
</p>

<h1>The Lottery Ticket Hypothesis</h1>
<p>This post aims to give a short survey of the lottery ticket
hypothesis (LTH), arguably the most fascinating topic in sparse neural
networks. I’ll assume basic familiarity with neural network pruning. If
you’re interested in learning more about neural network sparsity/pruning
in general, I can think of no better resource than <a
href="https://www.youtube.com/watch?v=H7-p3OWPpEI">this lecture</a>,
where Torsten Hoefler gives an extensive introduction on the field,
based on his 90-page <a href="https://arxiv.org/abs/2102.00554">survey
paper</a>.</p>
<p>For other sources on the LTH, I can recommend:</p>
<ul>
<li>The original papers on the LTH by Frankle et al. (<a
href="https://arxiv.org/abs/1803.03635">1</a>, <a
href="https://arxiv.org/abs/1912.05671">2</a>) are the standard
references. They are excellently written and contain <em>very</em>
in-depth appendices.</li>
<li><a
href="https://roberttlange.github.io/posts/2020/06/lottery-ticket-hypothesis/">A
survey by R. Lange</a>, summarizing a lot of the core papers on the
LTH.</li>
</ul>
<h2 id="cha:1">The lottery ticket hypothesis</h2>
<p>It was commonly believed that training a sparse neural network from
scratch is infeasible, until a landmark paper by <a
href="https://arxiv.org/abs/1803.03635">Frankle and Carbin (2018)</a>
introduced the lottery ticket hypothesis. Through extensive
experimentation, the authors show that a sparse network can train to the
same accuracy as a dense network, as long as a specific initialization
is used. Based on this observation, <a
href="https://arxiv.org/abs/1803.03635">Frankle and Carbin (2018)</a>
formulate the following hypothesis:</p>
<p><strong>The Lottery Ticket Hypothesis.</strong> <em>A
randomly-initialized, dense neural network contains a subnetwork that is
initialized such that — when trained in isolation — it can match the
test accuracy of the original network after training for at most the
same number of iterations.</em></p>
<p>This implies that, if only the correct subnet can be determined, we
can train the sparse subnet instead of the dense network. A hopeful
result, although the method to find the subnet in question is very
expensive. It consists of an iterative pruning algorithm, where
non-pruned weights are reset back to their initialization before
retraining. This is called iterative magnitude pruning (IMP).</p>
<blockquote>
<p><strong>Algorithm 1</strong>: Iterative magnitude pruning<br />
Initialize the network
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝒩</mi><annotation encoding="application/x-tex">\mathcal{N}</annotation></semantics></math>
with random weights
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝛉</mi><mn>0</mn></msub><annotation encoding="application/x-tex">\boldsymbol{\theta}_0</annotation></semantics></math>.
Create the mask
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐦</mi><annotation encoding="application/x-tex">\boldsymbol{m}</annotation></semantics></math>
:=
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mn>1</mn><mi>d</mi></msup><annotation encoding="application/x-tex">1^d</annotation></semantics></math>.<br />
<em>while</em> desired sparsity not achieved <em>do</em><br />
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mspace width="2.0em"></mspace><annotation encoding="application/x-tex">\qquad</annotation></semantics></math>Train
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝛉</mi><mn>0</mn></msub><mo>⊙</mo><mi>𝐦</mi></mrow><annotation encoding="application/x-tex">\boldsymbol{\theta}_0 \odot \boldsymbol{m}</annotation></semantics></math>
for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
iterations, arriving at weights
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝛉</mi><mi>i</mi></msub><mo>⊙</mo><mi>𝐦</mi></mrow><annotation encoding="application/x-tex">\boldsymbol{\theta}_i \odot \boldsymbol{m}</annotation></semantics></math>.<br />
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mspace width="2.0em"></mspace><annotation encoding="application/x-tex">\qquad</annotation></semantics></math>Prune
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mi>%</mi></mrow><annotation encoding="application/x-tex">p\%</annotation></semantics></math>
of the connections not masked by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐦</mi><annotation encoding="application/x-tex">\boldsymbol{m}</annotation></semantics></math>
with the smallest magnitude in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝛉</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\boldsymbol{\theta}_i</annotation></semantics></math>.
Set these connections to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>0</mn><annotation encoding="application/x-tex">0</annotation></semantics></math>
in the mask
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐦</mi><annotation encoding="application/x-tex">\boldsymbol{m}</annotation></semantics></math>.</p>
</blockquote>
<p><a href="#fig:basic_ResNet20_IMP">Figure 1</a> gives a basic
demonstration of IMP. At each pruning round, the trained weights are
used to determine the new sparser mask, while the remaining weights
always start from the same initialization. As a comparison, using the
exact same structure with a new initialization (usually called a
reinitialization, or reinit) reaches significantly lower performance.
This is the novel part of IMP compared to prior sparse training
attempts; the weights need to be the same as used when pruning.</p>
<figure id="fig:basic_ResNet20_IMP">
<center>
<img src="./../assets/lth/basic_ResNet20_IMP.png" height="300">
</center>
<figcaption>
<br><i>Figure 1:</i> Basic example of the lottery ticket hypothesis on a
residual network. A first experiment performs iterative magnitude
pruning. Each round the network is trained, after which the 20% smallest
weights are removed and the remaining weights are reset to the
initialization. In a second experiment we train the same network
structures, but with a new reinitialization instead of the original
initialization. Experiments on CIFAR-10 with ResNet20. Average of 3 runs
with different seeds; error bars indicate maximum and minimum values.
</figcaption>
</figure>
<p>The lottery ticket hypothesis also holds to some extent using
one-shot pruning, but <a href="https://arxiv.org/abs/1803.03635">Frankle
and Carbin (2018)</a> show that the strongest results are achieved when
iteratively pruning and retraining with an exponentially decaying
sparsity schedule. In practice, pruning
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>20</mn><mi>%</mi></mrow><annotation encoding="application/x-tex">20\%</annotation></semantics></math>
per round is the usual compromise between accuracy and speed. This means
that IMP is an exceedingly slow algorithm to prune a network; achieving
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>99</mn><mi>%</mi></mrow><annotation encoding="application/x-tex">99\%</annotation></semantics></math>
sparsity requires fully training the model 20 times (as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>0.8</mn><mn>20</mn></msup><mo>≈</mo><mn>0.01</mn></mrow><annotation encoding="application/x-tex">0.8^{20} \approx 0.01</annotation></semantics></math>).
This is further exacerbated by the fact that IMP is not
parallelizable.</p>
<p>The LTH sparked many follow-up questions. Why is the initialization
so important combined with the structure. What are the properties of
these lottery tickets? Does it hold in all cases? Are lottery tickets
specific to a certain dataset or optimizer?</p>
<p>In the original paper, <a
href="https://arxiv.org/abs/1803.03635">Frankle and Carbin (2018)</a>
conjecture (but do not prove) that SGD seeks out a small subnet (the
lottery ticket) during training and optimizes this. IMP finds these
small subnets and cuts away all the superfluous parts of the model,
which explains why very sparse networks can still rival the dense
performance. According to this interpretation, over-parameterized
networks are easier to train because they contain more subnets, and
hence have more possible winning tickets at initialization.</p>
<h2 id="linear-mode-connectivity">Linear mode connectivity</h2>
<p>To find lottery tickets in medium-sized networks <a
href="https://arxiv.org/abs/1803.03635">Frankle and Carbin (2018)</a>
need to use warm-up and a small learning rate. <a
href="https://arxiv.org/abs/1810.05270">Liu et al. (2018)</a> and <a
href="https://arxiv.org/abs/1902.09574">Gale et al. (2019)</a> show that
lottery tickets could not be found in large-scale networks such as
ResNet50 on ImageNet, further calling the generality of the LTH into
question. This prompted <a
href="https://arxiv.org/abs/1912.05671">Frankle et al. (2020)</a> to
devise a refinement to the lottery ticket hypothesis, relying upon the
concept of mode connectivity.</p>
<p>Mode connectivity is introduced to study the (in)stability of
stochastic training. Neural network optimization introduces noise into
training through the random ordering of batches and possibly also via
data augmentations (e.g. random flips of images). By training the
network twice from the same starting point with different noise, one can
determine if the resulting optima are connected as a proxy for
stability.</p>
<p>Two networks are called mode connected if there exists a path between
them in weight space that does not increase the error. Linear mode
connectivity requires the path between the modes to be linear.</p>
<p>More formally let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ℰ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝛉</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{E}(\boldsymbol{\theta})</annotation></semantics></math>
be the error of the network with weights
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛉</mi><annotation encoding="application/x-tex">\boldsymbol{\theta}</annotation></semantics></math>.
<a href="https://arxiv.org/abs/1912.05671">Frankle et al. (2020)</a>
define the linearly interpolated error between
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝛉</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\boldsymbol{\theta}_1</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝛉</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\boldsymbol{\theta}_2</annotation></semantics></math>
as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ℰ</mi><mi>α</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝛉</mi><mn>1</mn></msub><mo>,</mo><msub><mi>𝛉</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>ℰ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>α</mi><mo>⋅</mo><msub><mi>𝛉</mi><mn>1</mn></msub><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>α</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>⋅</mo><msub><mi>𝛉</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{E}_\alpha (\boldsymbol{\theta}_1, \boldsymbol{\theta}_2) = \mathcal{E}(\alpha \cdot \boldsymbol{\theta}_1 + (1-\alpha) \cdot \boldsymbol{\theta}_2)</annotation></semantics></math>,
with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>∈</mo><mrow><mo stretchy="true" form="prefix">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\alpha \in [0, 1]</annotation></semantics></math>.
The linear interpolation instability (LII) is the highest loss increase
on this path:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mi>I</mi><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝛉</mi><mn>1</mn></msub><mo>,</mo><msub><mi>𝛉</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><munder><mo>sup</mo><mi>α</mi></munder><mspace width="0.167em"></mspace><msub><mi>ℰ</mi><mi>α</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝛉</mi><mn>1</mn></msub><mo>,</mo><msub><mi>𝛉</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mfrac><mrow><mi>ℰ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝛉</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>ℰ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝛉</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mn>2</mn></mfrac></mrow><annotation encoding="application/x-tex">LII(\boldsymbol{\theta}_1, \boldsymbol{\theta}_2) = \sup_\alpha \, \mathcal{E}_\alpha (\boldsymbol{\theta}_1, \boldsymbol{\theta}_2) - \frac {\mathcal{E}(\boldsymbol{\theta}_1) + \mathcal{E}(\boldsymbol{\theta}_2)} 2</annotation></semantics></math></p>
<p>The LII is used to check the stability of networks towards SGD noise.
When training a network twice using a different data ordering, a network
is said to be stable to SGD noise when these two networks after training
have a LII close to zero. In practice, the LII will rarely be exactly
zero, so a small threshold such as 2% can be used as a measure for
empirical stability.</p>
<p><a href="https://arxiv.org/abs/1912.05671">Frankle et al. (2020)</a>
show that very small networks such as LeNet are already stable at
initialization, while larger networks become stable in an early phase
during training. <a href="#fig:full_instability_at_init">Figure 2</a>
shows the error on the linear path between two dense networks trained
from the same initialization with different SGD noise. All models except
LeNet are completely unstable at initialization, achieving random error
between the two modes. <a href="#fig:full_instability_later">Figure
3</a> shows how the stability evolves of dense models during training.
The LII becomes close to zero after a short period in early
training.</p>
<figure id="fig:full_instability_at_init">
<center>
<img src="../assets/lth/hill-full-zero/lenet.png" height="200">
<img src="../assets/lth/hill-full-zero/resnet20.png" height="200">
<img src="../assets/lth/hill-full-zero/vgg16.png" height="200">
<img src="../assets/lth/hill-full-zero/resnet50.png" height="200">
<img src="../assets/lth/hill-full-zero/inceptionv3.png" height="200">
</center>
<figcaption>
<br> <i>Figure 2:</i> Error on the linear path between networks trained
from the same initialization with different SGD noise. Trained networks
are at 0.0 and 1.0. Each line is the mean and standard deviation across
three initializations and three data orders (nine samples in total).
Reprinted from <a href="https://arxiv.org/abs/1912.05671">Frankle et
al. (2020)</a>.
</figcaption>
</figure>
<figure id="fig:full_instability_later">
<center>
<img src="../assets/lth/full-stability/lenet-level0-dataorder.png" height="200">
<img src="../assets/lth/full-stability/resnet20-level0-dataorder.png" height="200">
<img src="../assets/lth/full-stability/vgg16-level0-dataorder.png" height="200">
<img src="../assets/lth/full-stability/resnet50-level0-dataorder.png" height="200">
<img src="../assets/lth/full-stability/inceptionv3-level0-dataorder.png" height="200">
</center>
<figcaption>
<br> <i>Figure 3:</i> Linear interpolation instability when starting
from step k. Each line is the mean and standard deviation across three
initializations and three data orders (nine samples in total). Reprinted
from <a href="https://arxiv.org/abs/1912.05671">Frankle et
al. (2020)</a>.
</figcaption>
</figure>
<p><a href="https://arxiv.org/abs/1912.05671">Frankle et al. (2020)</a>
connect this discussion on stability with the LTH, by showing through
extensive experiments that the hypothesis only applies when resetting to
a stable state. So instead of resetting entirely to initialization each
round, they rewind to an early epoch where the training has become
stable. This revised version of the LTH is also called the stabilized
lottery ticket hypothesis.</p>
<p>Although they originally focussed on computer vision tasks,
subsequent works have since shown that the stabilized LTH holds on a
very wide range of models and tasks, such as BERT <a
href="https://arxiv.org/abs/2007.12223">(Chen et al, 2020)</a>,
self-supervised computer vision tasks <a
href="https://arxiv.org/abs/2012.06908">(Chen et al, 2021)</a>,
adversarial training <a href="https://arxiv.org/abs/2003.05733">(Li et
al, 2020)</a>, LSTM’s and reinforcement learning <a
href="https://arxiv.org/abs/1906.02768">(Yu et al, 2020)</a>.</p>
<h2 id="pruning-as-training-and-the-supermask">Pruning as training and
the supermask</h2>
<p>The reliance of the lottery tickets on the original initialization
leads to questions on the nature of these initializations. <a
href="https://arxiv.org/abs/1803.03635">Frankle and Carbin (2018)</a>
hypothesize that lottery tickets encode useful inductive biases for the
problem and the optimizer. However, <a
href="https://arxiv.org/abs/1905.01067">Zhou et al. (2019)</a> find that
lottery tickets can already exhibit non-trivial accuracy without
training, indicating that they contain a stronger prior than what <a
href="https://arxiv.org/abs/1803.03635">Frankle and Carbin (2018)</a>
implied.</p>
<p><a href="https://arxiv.org/abs/1905.01067">Zhou et al. (2019)</a>
continue by designing a training method that does not update weights,
but only removes them. Given the huge number of parameters in a dense
network, and that a network has in the order of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mn>2</mn><mrow><mo stretchy="true" form="prefix">|</mo><mi>𝛉</mi><mo stretchy="true" form="postfix">|</mo></mrow></msup><annotation encoding="application/x-tex">2^{|\boldsymbol{\theta}|}</annotation></semantics></math>
subnetworks, the authors propose that there exists a performant subnet
hidden in every randomly-initialized network. <a
href="https://arxiv.org/abs/1911.13299">Ramanujan et al. (2020)</a>
improve on this algorithm and show that even for more challenging tasks
such as ImageNet, they can find subnets with fairly competitive
accuracies. These performant subnets of random networks are called
supermasks. Based on this empirical evidence, <a
href="https://arxiv.org/abs/1911.13299">Ramanujan et al. (2020)</a>
state a stronger version of the LTH:</p>
<p><strong>The Strong Lottery Ticket Hypothesis.</strong> <em>A
randomly-initialized, dense neural network
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝒩</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\mathcal{N}_1</annotation></semantics></math>
contains a subnetwork
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝒩</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\mathcal{N}_2</annotation></semantics></math>
that is initialized such that — without training — it can match the test
accuracy of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝒩</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\mathcal{N}_1</annotation></semantics></math>
after training.</em></p>
<p>The supermask methods show that pruning can be regarded as a method
of training. Pruning simply constrains the hypothesis space to the
axis-parallel projections of the full network, which can still be vast
as long as the network is sufficiently over-parameterized. The work of
<a href="https://arxiv.org/abs/2002.00585">Malach et al. (2020)</a>
gives a theoretical foundation to this observation, by proving a
restricted version of the strong LTH. They prove that any dense ReLU
network of depth
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>,
can be approximated with an arbitrarly high probability, by a subnet of
a random
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>l</mi></mrow><annotation encoding="application/x-tex">2l</annotation></semantics></math>-layer
network with sufficient width. Contrary to the proposition of the strong
LTH in <a href="https://arxiv.org/abs/1911.13299">Ramanujan et
al. (2020)</a>, the proof requires polynomial over-parametrization of
the random network width, compared to the target network, limiting the
practical implications. <a
href="https://arxiv.org/abs/2006.12156">Orseau et al. (2020)</a> improve
this to a logarithmic bound, and removed the double depth
requirement.</p>
<p>Though pruning random initialized networks is not necessarily useful
in practice, it reshapes our understanding of pruning. Pruning is not
only a compression method but also expressive enough to be a training
method. This indicates that IMP might encode much bigger priors in
lottery tickets then the IMP algorithm would reveal at first glance.
This raises the concern that a lottery ticket might already contain all
information of the solution. In the next section, we discuss the
transferability of lottery tickets, to investigate how specifically
these are tailored to the task or optimization process.</p>
<h2 id="transferability-of-lottery-tickets">Transferability of lottery
tickets</h2>
<p>Finding lottery tickets using IMP can be exceedingly expensive. Hence
a natural question is whether a ticket transfers to other environments.
Can a lottery ticket found using a certain optimizer also train to
commensurate accuracy using another optimizer? And do they transfer
across datasets, or even tasks?</p>
<p><a href="https://arxiv.org/abs/1906.02773">Morcos et al. (2019)</a>
investigate precisely these questions on common vision tasks. They find
that lottery tickets transfer fairly well across optimizers and
datasets. Furthermore, winning tickets found on larger datasets
(e.g. ImageNet or Places365) perform consistently better on small vision
tasks (e.g. CIFAR10 or FashionMNIST), compared to the tickets generated
on these tasks themselves. <a
href="https://arxiv.org/abs/2005.05232">Sabatelli et al. (2020)</a> also
evaluate transferability on more diverse vision tasks with comparable
results.</p>
<p><a href="https://arxiv.org/abs/2007.12223">Chen et al. (2020)</a>
consider lottery ticket transferability in NLP settings. There is
already an established practice in NLP to pretrain large language models
such as BERT. The large upfront cost is amortized by transfer learning
to a variety of downstream tasks. <a
href="https://arxiv.org/abs/2007.12223">Chen et al. (2020)</a> propose
to use IMP in a similar fashion: the costly procedure of finding a
ticket is performed once on a masked language modeling task, after which
the ticket can be trained on a downstream task. They empirically show
that BERT has universal lottery tickets at 70% sparsity, meaning they
can transfer to tasks from GLUE and SQUAD while maintaining accuracy. In
contrast, tickets found on downstream tasks did not transfer to most
other tasks.</p>
<p>These results indicate that tickets found by IMP are reasonably
general; structural transfer learning gives similar results compared to
the established weight-based form of transfer learning.</p>

</body>
</html>