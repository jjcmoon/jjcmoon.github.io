<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Jaron Maene</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="canonical" href="http://maene.dev" />
  <link rel="stylesheet" href="style.css">
	
</head>


<body>
<hr>
  <div style="float: left;">
    <h2 style="margin: auto;">Jaron Maene</h2>
  </div>
  <div style="float: right;">
    <a href="mailto:jaron[dot]maene@kuleuven[dot]com">email</a> - 
    <a href="https://scholar.google.be/citations?user=leES0h0AAAAJ">scholar</a> - 
    <a href="https://github.com/jjcmoon">github</a> -
    <a href="https://bsky.app/profile/jjcmoon.bsky.social">bsky</a> - 
    <a href="https://www.linkedin.com/in/jaron-maene/">linkedin</a> 
  </div>
  <br><br>

<p>
  Hey there, I'm Jaron, a PhD candidate part of the <a href="https://wms.cs.kuleuven.be/dtai">DTAI</a> research group at <a href="https://www.kuleuven.be/english">KU Leuven</a>, supervised by <a href="https://wms.cs.kuleuven.be/people/lucderaedt">Luc De Raedt</a>. I do research in the intersection of probabilistic reasoning and deep learning (<em>neurosymbolic AI</em>).
</p>

<hr>

<h2> news </h2>


<table>
  <tr>
    <td>This summer I'll be on a research internship at <a href="https://basis.ai">Basis</a>, working on a probabilistic programming library called <i>Weighted</i>.</td>
    <td class="news">Jun 25</td>
  </tr>
  <tr>
    <td>Next semester I'll be at UCLA to visit <a href="https://web.cs.ucla.edu/~guyvdb/">Prof. Guy Van den Broeck</a>.</td>
    <td class="news">Feb 25</td>
  </tr>
  <tr>
    <td><i>"Extracting Finite State Machines from Transformers"</i> was accepted at the Workshop on Mechanistic Interpretability at ICML2024.<span class="paper-links inline"><a href="https://openreview.net/pdf?id=HeuQh5baef">paper</a></span></td>
    <td class="news">Jun 24</td>
  </tr>
  <tr>
    <td>Attended the AI winter school at Paderborn University.</td>
    <td class="news">Feb 24</td>
  </tr>
  <tr>
    <td>Attended the DeepLearn 2023 Winter school.</td>
    <td class="news">Jan 23</td>
  </tr>
  <tr>
    <td>I received a prize from IBM at the <a href="https://arxiv.org/abs/2103.02523">NLC2CMD competition</a> at NeurIPS20, for my work on program synthesis with LLMs during a research internship at Bell Labs. </td>
    <td class="news">Dec 20</td>
  </tr>
</table> 

<h2> selected papers </h2>

<table>
    <tr>
    <td><i>Embeddings as Probabilistic Equivalence in Logic Programs</i><div class="paper-summary">Proposes a distribution semantics of logic programming using probabilistic equivalence instead of probabilistic facts, leading to an end-to-end differentiable prover without local minima.</div></td>
    <td class="news">NeurIPS25</td>
  </tr>
  <tr>
    <td><i>KLay: Accelerating Arithmetic Circuits for Neurosymbolic AI</i><div class="paper-summary">Introduces knowledge layers (KLay), a new GPU-optimized library for evaluating sparse arithmetic circuits, achieving speedups of multiple orders of magnitude over existing methods.</div><span class="paper-links"><a href="https://openreview.net/pdf?id=Zes7Wyif8G">paper</a><a href="https://www.youtube.com/watch?v=9IYy6Sy852Y">video</a><a href="https://github.com/ML-KULeuven/klay">code</a></span></td>
    <td class="news">ICLR25</td>
  </tr>
    <tr>
    <td><i>The Gradient of Algebraic Model Counting</i> (Oral, top 4.6%)<div class="paper-summary">Extends algebraic model counting from inference to learning by generalizing gradients and backpropagation to different semirings, unifying various learning algorithms in neurosymbolic AI.</div><span class="paper-links"><a href="https://arxiv.org/pdf/2502.18406">paper</a><a href="https://www.youtube.com/watch?v=1PtfJ43RSKo">video</a><a href="https://github.com/ML-KULeuven/amc-grad">code</a></span></td>
    <td class="news">AAAI25</td>
  </tr>
    <tr>
    <td><i>On the Hardness of Probabilistic Neurosymbolic Learning</i><div class="paper-summary">Studies the computational complexity of differentiating probabilistic reasoning and introduces WeightME, an unbiased gradient estimator with provides probabilistic guarantees.</div><span class="paper-links"><a href="https://arxiv.org/pdf/2406.04472">paper</a><a href="https://www.youtube.com/watch?v=sWJU9dag5i8">video</a><a href="https://github.com/jjcmoon/hardness-nesy">code</a></span></td>
    <td class="news">ICML24</td>
  </tr>
    <tr>
    <td><i>Soft-Unification in Deep Probabilistic Programming</i><div class="paper-summary">Introduces DeepSoftLog, a principled probabilistic framework for soft-unification that addresses limitations in previous neural theorem provers and enables end-to-end differentiable logic rule learning.</div><span class="paper-links"><a href="https://openreview.net/pdf?id=s86M8naPSv">paper</a><a href="https://www.youtube.com/watch?v=3yQbcer-suA">video</a><a href="https://github.com/jjcmoon/DeepSoftLog">code</a></span>
    </td>
    <td class="news">NeurIPS23</td>
  </tr>



</table> 



<h2> appearances </h2>

<table>
  <tr>
    <td>I gave a talk a the StarAI lab at UCLA.</td>
    <td class="news">Nov 24</td>
  </tr>
  <tr>
    <td>Presented two posters at the first <a href="https://sites.google.com/view/nesy2024">NeSy conference</a>.</td>
    <td class="news">Sep 24</td>
  </tr>
  <tr>
    <td> I gave an invited seminar at the TU Wien Institute of Logic and Computation.</td>
    <td class="news">Jul 24</td>
  </tr>
  <tr>
    <td><i>"Soft-Unification in Deep Probabilistic Logic"</i>, talk at the <a href="https://sites.google.com/view/genesy2024/">Generative NeSy Workshop</a>.</td>
    <td class="news">May 24</td>
  </tr>
  <tr>
    <td><i>"Neurosymbolic Learning, a Probabilistic Journey"</i>, DTAI seminar.<span class="paper-links inline"><a href="assets/slides/probabilistic_nesy.pdf">slides</a></span></td>
    <td class="news"></td>
  </tr>
  <tr>
    <td><i>"AI and your Research"</i>, Invited talk (non-technical) at the LC&Y institute on the use of AI in research.<span class="paper-links inline"><a href="https://www.youtube.com/watch?v=OWFxO3Hb83Y">video</a></span></td>
    <td class="news">Mar 24</td>
  </tr>
  <tr>
    <td>Presented a poster at the Flanders AI research day 2023.</td>
    <td class="news">Nov 23</td>
  </tr>
</table> 

<h2> master students </h2>

<table>
    <tr>
    <td>Sam McManagan. <i>"A Neurosymbolic Approach to Solving Referring Expression Comprehension Tasks"</i></td>
    <td class="news">2025</td>
  </tr>
  <tr>
    <td>Andrei-Bogdan Florea. <i>"Think before you Learn: Image Segmentation with Weak Supervision"</i></td>
    <td class="news">2025</td>
  </tr>
  <tr>
    <td>Andres Van Schel. <i>"An Evaluation of Current Mechanistic Interpretability Techniques on an Entailment Prediction Task in Propositional Logic with BERT"</i></td>
    <td class="news">2025</td>
  </tr>
  <tr>
    <td>Wout Seynaeve. <i>"Learning from Logical Constraints: A Unifying approach for Weakly Supervised Semantic Segmentation"</i></td>
    <td class="news">2025</td>
  </tr>
    <tr>
    <td>Rik Adriaensen. <i>"Extracting Finite State Machines from Transformers"</i></td>
    <td class="news">2024</td>
  </tr>


</table> 

<h2>service</h2>
I have reviewed for JMLR, JAIR, ICLR25, and NeurIPS25.

<hr>

<div style="color:gray">
  <p>
    <em>The preceding merely defines a way of thinking. But the point is to live.</em><br>
    <span style="float: right;">â€” Albert Camus, Le Mythe de Sisyphe</span>
  </p>
Last updated: September, 2025.
</div>

<script data-goatcounter="https://jjcmoon.goatcounter.com/count"
        async src="//gc.zgo.at/count.js"></script>

</body>
</html>