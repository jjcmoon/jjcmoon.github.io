<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Jaron Maene</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="canonical" href="http://maene.dev" />
  <link rel="stylesheet" href="../style.css">
</head>


<body>
  <p style="text-align:left;">
    <a href="../index.html">Home</a>
    <span style="float:right;color:gray;">
        Written 
    </span>
</p>

<h1>Empirically Untangling the Meaning of Grok</h1>
<p>Heinlein’s 1961 novel “Stranger in a Strange Land” contains one of
the most influential sci-fi stories of the previous century. The
protagonist is Valentine Micheal Smith, a man that was raised by aliens
on Mars. The story starts when he returns to Earth and is confronted
with human culture. Especially the second half of the book is kind of
controversial (Spoiler alert: Micheal founds a sex cult that gives you
magical powers), but the book nonetheless contains a lot of interesting
concepts that were ahead of its time. The most influential of which is
probably the Martian word “grok”.</p>
<p>Grok is frequently used by Micheal (and Martians in general). But
Micheal can’t give a good explanation of its meaning; it’s an <a
href="https://en.wikipedia.org/wiki/Untranslatability#Vocabulary">untranslatable
word</a>. Eventually, we learn that the literal meaning of the Martian
word is “to drink”. However, the term is used in a much broader way.
Drinking is a metaphor for a kind of absorption of knowledge. Hence it
might be better to translate it as “to understand” or “to embrace”, but
the book emphasizes that grokking is a deeper concept than
understanding. It is a central concept in Martian culture. Some passages
from the book:</p>
<blockquote>
<p>Grok means “to understand,” of course, but Dr. Mahmoud, who might be
termed the leading Terran expert on Martians, explains that it also
means, “to drink” and “a hundred other English words, words which we
think of as antithetical concepts. ‘Grok’ means all of these. It means
‘fear,’ it means ‘love,’ it means ‘hate’—proper hate, for by the Martian
‘map’ you cannot hate anything unless you grok it, understand it so
thoroughly that you merge with it and it merges with you—then you can
hate it. By hating yourself. But this implies that you love it, too, and
cherish it and would not have it otherwise.</p>
</blockquote>
<blockquote>
<p>‘Grok’ means to understand so thoroughly that the observer becomes a
part of the observed—to merge, blend, intermarry, lose identity in group
experience. It means almost everything that we mean by religion,
philosophy, and science and it means as little to us as color does to a
blind man.</p>
</blockquote>
<p>However much Heinlein wants to cover the definition of the word in a
shroud of new-age mystery, the meaning of a word is a function of its
usage, not its definition. Hence, I would like to try to empirically
evaluate its functional meaning, by using BERT.</p>
<p><a href="http://jalammar.github.io/illustrated-bert/">BERT</a> is a
masked language model, meaning that it predicts the likelihood of a word
appearing in a given context.</p>
<figure>
<img src="../assets/grokking/BERT.png" alt="png" />
<figcaption aria-hidden="true">png</figcaption>
</figure>
<p>This allows to approximate what English words are used most similarly
to “grok”. For example, if we mask grok in the following context:</p>
<blockquote>
<p>“But, Jubal, don’t make a suggestion like that to Mike. He wouldn’t
[MASK] that you were joking - and you might have a corpse on your
hands.”</p>
</blockquote>
<p>BERT’s top 2 predictions are: “believe” and “understand”.</p>
<h1 id="experiments">Experiments</h1>
<p>We can get a more complete picture by averaging the results for all
the 305 occurrences of grok in “Stranger in a strange land” (the
original, uncut version). These predictions were generated using the
pre-trained huggingface model called <code>roberta-large</code> (a
variant of BERT).</p>
<table>
<thead>
<tr class="header">
<th>word</th>
<th>probability</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>know</td>
<td>11.67%</td>
</tr>
<tr class="even">
<td>understand</td>
<td>10.96%</td>
</tr>
<tr class="odd">
<td>think</td>
<td>7.28%</td>
</tr>
<tr class="even">
<td>see</td>
<td>3.73%</td>
</tr>
<tr class="odd">
<td>do</td>
<td>2.78%</td>
</tr>
<tr class="even">
<td>get</td>
<td>1.48%</td>
</tr>
<tr class="odd">
<td>believe</td>
<td>1.39%</td>
</tr>
</tbody>
</table>
<p>I only included the words that have probabilities above 1.2%. We can
repeat the same exercise with “grokked” (102 occurrences):</p>
<table>
<thead>
<tr class="header">
<th>word</th>
<th>probability</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>knew</td>
<td>7.07%</td>
</tr>
<tr class="even">
<td>understood</td>
<td>6.90%</td>
</tr>
<tr class="odd">
<td>saw</td>
<td>4.29%</td>
</tr>
<tr class="even">
<td>realized</td>
<td>3.01%</td>
</tr>
<tr class="odd">
<td>thought</td>
<td>2.72%</td>
</tr>
<tr class="even">
<td>felt</td>
<td>2.24%</td>
</tr>
<tr class="odd">
<td>learned</td>
<td>2.03%</td>
</tr>
<tr class="even">
<td>seen</td>
<td>1.99%</td>
</tr>
<tr class="odd">
<td>known</td>
<td>1.59%</td>
</tr>
</tbody>
</table>
<p>Reassuringly, these give very similar results. Lastly, let’s check
the results for “grokking” (44 occurrences):</p>
<table>
<thead>
<tr class="header">
<th>word</th>
<th>probability</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>it</td>
<td>3.91%</td>
</tr>
<tr class="even">
<td>understanding</td>
<td>3.46%</td>
</tr>
<tr class="odd">
<td>learning</td>
<td>2.98%</td>
</tr>
<tr class="even">
<td>living</td>
<td>2.35%</td>
</tr>
<tr class="odd">
<td>shape</td>
<td>1.89%</td>
</tr>
<tr class="even">
<td>shape</td>
<td>1.47%</td>
</tr>
<tr class="odd">
<td>moment</td>
<td>1.33%</td>
</tr>
<tr class="even">
<td>natural</td>
<td>1.31%</td>
</tr>
<tr class="odd">
<td>doing</td>
<td>1.29%</td>
</tr>
<tr class="even">
<td>meaning</td>
<td>1.22%</td>
</tr>
</tbody>
</table>
<h1 id="discussion">Discussion</h1>
<p>The results of “grok” and “grokked” largely seem to agree with the <a
href="https://www.merriam-webster.com/dictionary/grok">definition in the
Merriam-Webster dictionary</a>: “to understand profoundly and
intuitively”. Especially so for the past tense. Only in the
decomposition of “grokking” do we get any hint of a more mystical
component. The active form of “grok” is related to e.g. “living”,
“shape” and “moment”. Note furthermore that the absolute probabilities
for the best predictions are much lower. This is what you would expect
if there is no single English word with a consistent match.</p>
<p>As a kind of control, I checked what the model predicts for regular
English words. If there are enough occurrences, the model predicts the
word itself by quite a large margin, followed by some related words. For
example, using “have” (1062 occurrences).</p>
<table>
<thead>
<tr class="header">
<th>word</th>
<th>probability</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>have</td>
<td>77.36%</td>
</tr>
<tr class="even">
<td>’ve</td>
<td>4.03%</td>
</tr>
<tr class="odd">
<td>need</td>
<td>1.52%</td>
</tr>
<tr class="even">
<td>get</td>
<td>1.32%</td>
</tr>
</tbody>
</table>
<p>Or for “understand” (85 occurrences)</p>
<table>
<thead>
<tr class="header">
<th>word</th>
<th>probability</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>understand</td>
<td>36.75%</td>
</tr>
<tr class="even">
<td>know</td>
<td>11.70%</td>
</tr>
<tr class="odd">
<td>see</td>
<td>4.52%</td>
</tr>
<tr class="even">
<td>get</td>
<td>3.81%</td>
</tr>
<tr class="odd">
<td>hear</td>
<td>3.05%</td>
</tr>
<tr class="even">
<td>believe</td>
<td>2.40%</td>
</tr>
</tbody>
</table>
<p>Notice that, especially when getting to the bottom of the table,
there start to appear entries that have less to do with “understand”,
like “hear”. It’s important to remember the semantics of this table.
It’s “can be used in the same context”, not “means the same thing”. Of
course, as the context length increases in size, and if the models are
accurate enough, these should largely converge. But alas, current models
and context sizes are not perfect. As an example, observe the output for
“first” (245 occurrences).</p>
<table>
<thead>
<tr class="header">
<th>word</th>
<th>probability</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>first</td>
<td>53.93%</td>
</tr>
<tr class="even">
<td>last</td>
<td>2.54%</td>
</tr>
<tr class="odd">
<td>next</td>
<td>1.52%</td>
</tr>
<tr class="even">
<td>second</td>
<td>1.18%</td>
</tr>
</tbody>
</table>
<p>Although it rightly guesses “first” with a big margin, the next
entries are not synonyms at all. Let’s take an example from the
book:</p>
<blockquote>
<p>Meet me at the desk; I’ll be paying the bill.” She left very
suddenly.. . They went to the town’s station flat and caught the [MASK]
Greyhound going anywhere.</p>
</blockquote>
<p>Observe how in this case, both “first”, and “next” are completely
sensible options. There is not enough context to decide. This explains
why for “grokking”, “it” is the first option. It can be used as a
generic placeholder for a word if there is no better alternative.
Similarly, “get” occurs quite often.</p>
<h1 id="replication">Replication</h1>
<p>All source code can be found in <a
href="https://colab.research.google.com/drive/1ipZGpoz_H1__SsqgO_bbEqUCExKOOB5f?usp=sharing">this
colab notebook</a>. To replicate you only need to upload “Stranger in a
strange land” in the epub format and rename it <code>book.epub</code>.
Next, you can just run all cells.</p>

</body>
</html>